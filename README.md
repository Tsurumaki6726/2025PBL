# News to Chat App

ニュース記事を「先生」と「生徒」の会話形式に自動変換するWebアプリケーション

## 概要

このアプリは、ニュース記事（CSV形式）をLLM（大規模言語モデル）で処理し、分かりやすい会話形式に変換します。

- **フロントエンド**: Next.js + React + Tailwind CSS
- **バックエンド**: Python + FastAPI + Swallow LLM (8B)
- **実行環境**: Google Colab (GPU)

## システム構成

### アーキテクチャ図

```
┌─────────────────────────────────────────────────────────────┐
│                     ユーザーのブラウザ                          │
│                  http://localhost:3000                       │
└──────────────────────────┬──────────────────────────────────┘
                           │
                    ┌──────▼────────┐
                    │  フロントエンド   │
                    │   (Next.js)   │
                    │  • React UI   │
                    │  • Tailwind   │
                    │  • shadcn/ui  │
                    └──────┬────────┘
                           │
              ┌────────────▼─────────────┐
              │ localtunnel経由のHTTP接続  │
              │  (HTTPS自動暗号化)        │
              └────────────┬─────────────┘
                           │
              ┌────────────▼──────────────────┐
              │   Google Colab バックエンド     │
              │  • FastAPI サーバー           │
              │  • GPU (T4/A100)             │
              │  • Swallow LLM (8B/13B)      │
              │  • transformers + unsloth    │
              └────────────┬──────────────────┘
                           │
              ┌────────────▼──────────────┐
              │   記事データ処理            │
              │  • CSV解析 (honbun, midasi) │
              │  • テキスト前処理           │
              │  • LLM推論実行             │
              │  • 会話形式変換             │
              └────────────┬──────────────┘
                           │
              ┌────────────▼──────────────┐
              │   処理結果を返却             │
              │  • 要約テキスト             │
              │  • 先生・生徒の会話JSON      │
              └───────────────────────────┘
```

### 各コンポーネントの詳細

#### フロントエンド (Next.js)

| 機能 | ファイル | 説明 |
|------|--------|------|
| メイン画面 | `app/page.tsx` | レイアウト定義、メイン処理 |
| サイドバー | `components/sidebar.tsx` | 接続状態、URL入力、切断 |
| 記事選択 | `components/article-selector.tsx` | 記事一覧表示 |
| CSV アップロード | `components/csv-uploader.tsx` | ファイルアップロード機能 |
| チャット表示 | `components/chat-area.tsx` | 変換結果の会話表示 |
| 変換ボタン | `components/convert-button.tsx` | API呼び出し実行 |

#### バックエンド (Google Colab)

| 機能 | 言語 | 説明 |
|------|------|------|
| メインスクリプト | Python | `news_to_chat_colab.py` |
| LLMセットアップ | Python | Swallow モデルロード、量子化 |
| FastAPI | Python | HTTP エンドポイント提供 |
| localtunnel | Node.js | URL公開、HTTPS暗号化 |

#### データフロー

1. **ユーザーがCSVをアップロード** → フロントエンドで受け取り
2. **記事を選択して変換実行** → FastAPI に POST リクエスト
3. **バックエンド処理** → LLM推論で会話を生成
4. **結果を返却** → JSON形式で会話データを返す
5. **フロントエンド表示** → 先生と生徒の会話を表示
```

## 初期セットアップ

### 前提条件

- **Node.js 18以上** - フロントエンド実行用
- **Google アカウント** - Colab用（無料）
- **アカウント登録不要**: localtunnelは登録なしで使えます

### ステップ1: リポジトリの準備

```bash
# リポジトリをクローン
git clone <このリポジトリのURL>
cd 2025PBL
```

### ステップ2: フロントエンドのセットアップ

```bash
# 依存関係のインストール
npm install
```

### ステップ3: フロントエンドの起動

```bash
# 開発サーバーを起動
npm run dev
```

ブラウザで `http://localhost:3000` にアクセスできることを確認

### ステップ4: Google Colab の準備

#### 4-1. Colabノートブックを開く

配布された `news_to_chat_colab.py` をGoogle Colabで開く

#### 4-2. 記事CSVを準備

記事データをCSV形式で準備（必須カラム: `honbun`, `midasi`）
ehime_kiji_001.csvをダウンロードすればOK

例:
```csv
honbun,midasi
"記事本文の内容がここに入ります...", "記事の見出し"
"別の記事本文...", "別の見出し"
```

#### 4-3. Colabを実行

1. 「ランタイム」→「すべてのセルを実行」
2. 最後のセルで **localtunnel URL** が表示される（例: `https://xxxx.loca.lt`）
3. このURLをコピー
4. URLをwebの検索欄に貼り付け
5. パスワードを入力してclick to submitを押下

## 使い方

### アプリの基本的な使用方法

1. フロントエンド（`http://localhost:3000`）を開く
2. 入力欄に **localtunnel URL** を貼り付け
3. 「接続」をクリック
4. **CSVファイルをアップロード**（フロントエンドから直接アップロード可能）
5. 記事を選択して「会話形式に変換」をクリック

### 基本的な流れ

1. **記事を選択** - 左パネルの記事一覧から記事をクリック
2. **内容を確認** - 選択した記事の本文がプレビュー表示される
3. **変換実行** - 「会話形式に変換」ボタンをクリック
4. **結果を確認** - 右パネルに要約と会話が表示される（10〜30秒）

### 会話の登場人物

| キャラクター | 役割 | 口調の例 |
|-------------|------|----------|
| **先生** | 解説役 | 「〜ですね」「〜なんです」「〜しましょう」 |
| **生徒** | 質問役 | 「〜ですか？」「〜なんですね」 |

### 複数記事の変換

- 別の記事を選択して、再度「会話形式に変換」をクリック
- 前の変換結果は上書きされます

### 接続の切断

- 左サイドバーの「切断」ボタンで接続を切断できます
- 保存されたURLもクリアされます

## トラブルシューティング

### 接続エラー

| 症状 | 原因 | 対処法 |
|------|------|--------|
| 「接続できません」 | Colabが起動していない | Colabで「すべてのセルを実行」 |
| 「接続できません」 | URLが間違っている | localtunnel URLを再確認 |
| 「接続できません」 | セッションが切れた | Colabを再実行して新しいURLを取得 |

### CSVアップロードエラー

| 症状 | 原因 | 対処法 |
|------|------|--------|
| アップロード失敗 | ファイル形式が間違っている | UTF-8エンコードのCSVか確認 |
| 記事が表示されない | `honbun`カラムがない | カラム名を確認（大文字小文字も区別） |
| 文字化け | エンコーディングが違う | UTF-8で保存し直す |

### パフォーマンス問題

| 症状 | 原因 | 対処法 |
|------|------|--------|
| メモリエラー | GPU制限に達した | 時間を置いてから再実行 |
| 処理が遅い | 記事が長すぎる | 記事を短く編集 |
| Colabが切断 | アイドル時間超過 | 定期的に操作する |

### よくある質問

**Q: localtunnel URLは毎回変わりますか？**
A: Colabを再起動すると変わる可能性があります。アプリは前回のURLを記憶して自動再接続を試みます。

**Q: CSVを複数アップロードできますか？**
A: 可能ですが、前のデータは上書きされます。

**Q: Google Colab無料版でどのくらい使えますか？**
A: GPU使用時間に制限があり、連続使用すると一時的に制限される場合があります。

## モデルの変更

より高性能なモデルや軽量なモデルに変更できます。

### 変更方法

Colabノートブックの `MODEL_ID` を変更：

\`\`\`python
# デフォルト（8B）
MODEL_ID = "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3"

# より高性能（13B）- メモリ使用量が増加
MODEL_ID = "tokyotech-llm/Llama-3.1-Swallow-13B-Instruct-v0.3"
\`\`\`

### モデルサイズとメモリ

| モデルサイズ | 4bit量子化後 | Colab無料版 |
|-------------|-------------|-----------|
| 7B〜8B | 約4GB | ✅ 推奨 |
| 13B | 約7GB | ⚠️ ギリギリ |
| 22B以上 | 約11GB以上 | ❌ 不可 |

## 注意事項

- Google Colab無料版ではGPU使用時間に制限があります
- Colabセッションは最大12時間、アイドル時間90分で切断されます
- localtunnelは登録不要ですが、セッションごとにURLが変わります
- 記事データに個人情報が含まれる場合は取り扱いに注意してください


## ライセンス

このプロジェクトは無償提供を前提としています。

### コードのライセンス

- **フロントエンド**: MIT License
  - Next.js, React, Tailwind CSS, shadcn/ui など
- **バックエンド**: MIT / Apache 2.0 License
  - FastAPI, unsloth, transformers など

### LLMモデルのライセンス

- **Swallow (Llama 3.1 based)**: Llama 3.1 Community License
  - ライセンス詳細: https://llama.meta.com/llama3/license/
  - 非商用利用であれば制限なく使用可能
  - 商用利用の場合は月間アクティブユーザー（MAU）が7億未満まで可能




