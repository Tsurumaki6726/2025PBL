# News to Chat App

ニュース記事を「先生」と「生徒」の会話形式に自動変換するWebアプリケーション

## 概要

このアプリは、ニュース記事（CSV形式）をLLM（大規模言語モデル）で処理し、分かりやすい会話形式に変換します。

- **フロントエンド**: Next.js + React + Tailwind CSS
- **バックエンド**: Python + FastAPI + Swallow LLM (8B)
- **実行環境**: Google Colab (GPU)

## システム構成

\`\`\`
フロントエンド (Next.js)
    ↓ HTTP接続（localtunnel経由）
バックエンド (Google Colab + GPU)
    ↓ CSVアップロード（フロントエンドから）
記事データ (honbun, midasi)
\`\`\`

## 初期セットアップ

### 前提条件

- **Node.js 18以上** - フロントエンド実行用
- **Google アカウント** - Colab用（無料）
- **アカウント登録不要**: localtunnelは登録なしで使えます

### ステップ1: リポジトリの準備

\`\`\`bash
# リポジトリをクローン
git clone <このリポジトリのURL>
<<<<<<< HEAD
cd news-to-chat-app
=======
cd 2025PBL
```
>>>>>>> d3d9618a3b9e8fff832d8e6a88a37f9c5b024518

# 依存関係のインストール
npm install
\`\`\`

### ステップ2: フロントエンドの起動

\`\`\`bash
# 開発サーバーを起動
npm run dev
\`\`\`

ブラウザで `http://localhost:3000` にアクセスできることを確認

### ステップ3: Google Colab の準備

#### 3-1. Colabノートブックを開く

1. 配布された `news_to_chat_colab.ipynb` をGoogle Colabで開く
2. 「ランタイム」→「ランタイムのタイプを変更」→ **GPU** を選択

#### 3-2. Colabを実行

<<<<<<< HEAD
1. 「ランタイム」→「すべてのセルを実行」をクリック
2. 実行には5〜10分かかります（初回はモデルダウンロードで時間がかかる）
3. 最後のセルで **localtunnel URL** が表示されます
=======
記事データをCSV形式で準備（必須カラム: `honbun`, `midasi`）
ehime_kiji_001.csvをダウンロードすればOK
>>>>>>> d3d9618a3b9e8fff832d8e6a88a37f9c5b024518

\`\`\`
Running on public URL: https://xxxx-xxxx-xxxx.loca.lt
\`\`\`

このURLをコピーしてください。

<<<<<<< HEAD
### ステップ4: フロントエンドとバックエンドの接続
=======
1. 「ランタイム」→「すべてのセルを実行」
2. 最後のセルで **localtunnel URL** が表示される（例: `https://xxxx.loca.lt`）
3. このURLをコピー
4. URLをwebの検索欄に貼り付け
5. パスワードを入力してclick to submitを押下
>>>>>>> d3d9618a3b9e8fff832d8e6a88a37f9c5b024518

#### 初回接続

1. フロントエンド（`http://localhost:3000`）を開く
<<<<<<< HEAD
2. 左側のサイドバーに **localtunnel URL** を貼り付け
3. 「接続」ボタンをクリック
4. 接続成功すると記事アップロード画面が表示されます

#### 2回目以降

- 前回と同じlocaltunnel URLの場合、**自動で再接続**されます
- URLが変わった場合は手動で新しいURLを入力してください

### ステップ5: CSVファイルの準備とアップロード

#### CSVファイル形式

必須カラム: `honbun`（記事本文）
任意カラム: `midasi`（記事見出し）

**例:**
\`\`\`csv
honbun,midasi
"東京都は本日、新型コロナウイルスの感染者数が100人を超えたと発表した。専門家は警戒を呼びかけている。","東京で感染者急増"
"気象庁によると、週末は全国的に晴れの見込み。","週末は晴天予報"
\`\`\`

#### アップロード方法

1. 接続後、左パネルの「Upload CSV File」セクションを探す
2. 「ファイルを選択」ボタンをクリック
3. 準備したCSVファイルを選択
4. 「アップロード」ボタンをクリック
5. 記事一覧が表示されることを確認
=======
2. 入力欄に **localtunnel URL** を貼り付け
3. 「接続」をクリック
4. **CSVファイルをアップロード**（フロントエンドから直接アップロード可能）
5. 記事を選択して「会話形式に変換」をクリック
>>>>>>> d3d9618a3b9e8fff832d8e6a88a37f9c5b024518

## 使い方

### 基本的な流れ

1. **記事を選択** - 左パネルの記事一覧から記事をクリック
2. **内容を確認** - 選択した記事の本文がプレビュー表示される
3. **変換実行** - 「会話形式に変換」ボタンをクリック
4. **結果を確認** - 右パネルに要約と会話が表示される（10〜30秒）

### 会話の登場人物

| キャラクター | 役割 | 口調の例 |
|-------------|------|----------|
| **先生** | 解説役 | 「〜ですね」「〜なんです」「〜しましょう」 |
| **生徒** | 質問役 | 「〜ですか？」「〜なんですね」 |

### 複数記事の変換

- 別の記事を選択して、再度「会話形式に変換」をクリック
- 前の変換結果は上書きされます

### 接続の切断

- 左サイドバーの「切断」ボタンで接続を切断できます
- 保存されたURLもクリアされます

## トラブルシューティング

### 接続エラー

| 症状 | 原因 | 対処法 |
|------|------|--------|
| 「接続できません」 | Colabが起動していない | Colabで「すべてのセルを実行」 |
| 「接続できません」 | URLが間違っている | localtunnel URLを再確認 |
| 「接続できません」 | セッションが切れた | Colabを再実行して新しいURLを取得 |

### CSVアップロードエラー

| 症状 | 原因 | 対処法 |
|------|------|--------|
| アップロード失敗 | ファイル形式が間違っている | UTF-8エンコードのCSVか確認 |
| 記事が表示されない | `honbun`カラムがない | カラム名を確認（大文字小文字も区別） |
| 文字化け | エンコーディングが違う | UTF-8で保存し直す |

### パフォーマンス問題

| 症状 | 原因 | 対処法 |
|------|------|--------|
| メモリエラー | GPU制限に達した | 時間を置いてから再実行 |
| 処理が遅い | 記事が長すぎる | 記事を短く編集 |
| Colabが切断 | アイドル時間超過 | 定期的に操作する |

### よくある質問

**Q: localtunnel URLは毎回変わりますか？**
A: Colabを再起動すると変わる可能性があります。アプリは前回のURLを記憶して自動再接続を試みます。

**Q: CSVを複数アップロードできますか？**
A: 可能ですが、前のデータは上書きされます。

**Q: Google Colab無料版でどのくらい使えますか？**
A: GPU使用時間に制限があり、連続使用すると一時的に制限される場合があります。

## モデルの変更

より高性能なモデルや軽量なモデルに変更できます。

### 変更方法

Colabノートブックの `MODEL_ID` を変更：

\`\`\`python
# デフォルト（8B）
MODEL_ID = "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3"

# より高性能（13B）- メモリ使用量が増加
MODEL_ID = "tokyotech-llm/Llama-3.1-Swallow-13B-Instruct-v0.3"
\`\`\`

### モデルサイズとメモリ

| モデルサイズ | 4bit量子化後 | Colab無料版 |
|-------------|-------------|-----------|
| 7B〜8B | 約4GB | ✅ 推奨 |
| 13B | 約7GB | ⚠️ ギリギリ |
| 22B以上 | 約11GB以上 | ❌ 不可 |

## 注意事項

### Google Colabの制限

<<<<<<< HEAD
- **セッション時間**: 最大12時間
- **アイドル時間**: 90分で自動切断
- **GPU制限**: 連続使用で一時的に制限される場合あり

### データ取り扱い

- 個人情報を含む記事データは慎重に扱ってください
- localtunnel URLは他人に共有しないでください
- 記事の著作権は元の著作者に帰属します

### セキュリティ

- このアプリは内部利用を想定しています
- localtunnel URLが漏れると第三者がアクセス可能です
- 機密情報を含むデータは使用しないでください

## デプロイ（オプション）

### フロントエンドのデプロイ

Vercelへデプロイ可能：

\`\`\`bash
# Vercel CLIでデプロイ
npm i -g vercel
vercel
\`\`\`

または、v0.appの「Publish」ボタンからワンクリックデプロイ。

**注意**: フロントエンドをデプロイしても、バックエンド（Colab）は各自で起動する必要があります。
=======
>>>>>>> d3d9618a3b9e8fff832d8e6a88a37f9c5b024518

## ライセンス

このプロジェクトは無償提供を前提としています。

### コードのライセンス

- **フロントエンド**: MIT License
  - Next.js, React, Tailwind CSS, shadcn/ui など
- **バックエンド**: MIT / Apache 2.0 License
  - FastAPI, unsloth, transformers など

### LLMモデルのライセンス

- **Swallow (Llama 3.1 based)**: Llama 3.1 Community License
  - ライセンス詳細: https://llama.meta.com/llama3/license/
  - 非商用利用であれば制限なく使用可能
  - 商用利用の場合は月間アクティブユーザー（MAU）が7億未満まで可能




