# ============================================================
# News to Chat API - Google Colabç‰ˆ
# Swallow v0.3 (Llama 3.1ãƒ™ãƒ¼ã‚¹) ã‚’ä½¿ç”¨ã—ãŸä¼šè©±å¤‰æ›
# ============================================================

# ============================================================
# ã€ã‚»ãƒ«1ã€‘ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆåˆå›ã®ã¿ï¼‰
# ============================================================

"""
!pip install unsloth
!pip install xformers
!pip install trl peft accelerate bitsandbytes
!pip install pandas
!pip install fastapi uvicorn
!npm install -g localtunnel
"""

# ============================================================
# ã€ã‚»ãƒ«2ã€‘ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ============================================================

from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import time
import torch
import pandas as pd
from unsloth import FastLanguageModel
import io

print("âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†")

# ============================================================
# ã€ã‚»ãƒ«3ã€‘è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®åˆæœŸåŒ–
# ============================================================

# è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆï¼ˆåˆæœŸã¯ç©ºï¼‰
ARTICLES = []

print("âœ… è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®åˆæœŸåŒ–å®Œäº†ï¼ˆCSVã¯ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼‰")

# ============================================================
# ã€ã‚»ãƒ«4ã€‘Swallowãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆ3ã€œ5åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰
# ============================================================

MODEL_ID = "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3"

print(f"â³ Swallowãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
print("â€»åˆå›ã¯3ã€œ5åˆ†ã‹ã‹ã‚Šã¾ã™ã€‚ãŠå¾…ã¡ãã ã•ã„...")

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=MODEL_ID,
    max_seq_length=4096,
    dtype=None,
    load_in_4bit=True,
)
FastLanguageModel.for_inference(model)

print("âœ… Swallowãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼")

# ============================================================
# ã€ã‚»ãƒ«5ã€‘ç”Ÿæˆé–¢æ•°ã®å®šç¾©
# ============================================================

def parse_conversation(text: str) -> list:
    """ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›"""
    lines = text.strip().split("\n")
    conversation = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith("å…ˆç”Ÿ:") or line.startswith("å…ˆç”Ÿï¼š"):
            content = line.replace("å…ˆç”Ÿ:", "").replace("å…ˆç”Ÿï¼š", "").strip()
            if content:
                conversation.append({"role": "character_a", "content": content})
        elif line.startswith("ç”Ÿå¾’:") or line.startswith("ç”Ÿå¾’ï¼š"):
            content = line.replace("ç”Ÿå¾’:", "").replace("ç”Ÿå¾’ï¼š", "").strip()
            if content:
                conversation.append({"role": "character_b", "content": content})
    
    return conversation


def process_article(article_id: int) -> dict:
    """è¨˜äº‹ã‚’è¦ç´„ã—ã€ä¼šè©±å½¢å¼ã«å¤‰æ›ã™ã‚‹"""
    
    if article_id < 0 or article_id >= len(ARTICLES):
        raise Exception(f"ç„¡åŠ¹ãªè¨˜äº‹ID: {article_id}")
    
    original_text = ARTICLES[article_id]["honbun"]
    start_time = time.time()
    
    # --- ã‚¹ãƒ†ãƒƒãƒ—A: è¦ç´„ ---
    summary_prompt = [
        {
            "role": "system",
            "content": "ã‚ãªãŸã¯å„ªç§€ãªç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®è¦ç‚¹ã‚’ã€äº‹å®Ÿã«åŸºã¥ã„ã¦200æ–‡å­—ç¨‹åº¦ã®æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚"
        },
        {
            "role": "user",
            "content": f"## è¨˜äº‹æœ¬æ–‡\n{original_text}"
        }
    ]
    
    inputs1 = tokenizer.apply_chat_template(
        summary_prompt,
        add_generation_prompt=True,
        tokenize=True,
        return_tensors="pt"
    ).to(model.device)
    
    outputs1 = model.generate(
        inputs1,
        max_new_tokens=300,
        temperature=0.3,
        use_cache=True
    )
    summary_text = tokenizer.decode(
        outputs1[0][inputs1.shape[-1]:],
        skip_special_tokens=True
    )
    
    # --- ã‚¹ãƒ†ãƒƒãƒ—B: ä¼šè©±å¤‰æ› ---
    roleplay_prompt = [
        {
            "role": "system",
            "content": """
ã‚ãªãŸã¯ãƒ—ãƒ­ã®è„šæœ¬å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ã€è¦ç´„ã€‘ã®å†…å®¹ã‚’ã€äºŒäººã®ç™»å ´äººç‰©ã®ä¼šè©±åŠ‡ï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼‰ã«æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚

ã€ç™»å ´äººç‰©ã€‘
1. **å…ˆç”Ÿ**: ä¸å¯§èªï¼ˆã€œã§ã™ã€ã€œã¾ã™ã­ã€ã€œã§ã—ã‚‡ã†ï¼‰ã§è©±ã™ã€‚è¦ªã—ã¿ã‚„ã™ãåˆ†ã‹ã‚Šã‚„ã™ãæ•™ãˆã‚‹æ•™å¸«ã€‚
2. **ç”Ÿå¾’**: ä¸å¯§èªï¼ˆã€œã§ã™ã­ã€ã€œã§ã™ã‹ï¼Ÿï¼‰ã§è©±ã™ã€‚æ•™ãˆã‚’ä¹ã†å­¦ç¿’è€…ã€‚

ã€æ§‹æˆãƒ«ãƒ¼ãƒ«ã€‘
- æŒ¨æ‹¶ã¯çœç•¥ã—ã€ç”Ÿå¾’ãŒè¨˜äº‹ã®å†…å®¹ã«ã¤ã„ã¦è³ªå•ã™ã‚‹ã¨ã“ã‚ã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨ã€‚
- å…ˆç”ŸãŒä¸å¯§ã«è§£èª¬ã—ã€ç”Ÿå¾’ãŒç´å¾—ã™ã‚‹æµã‚Œã«ã™ã‚‹ã“ã¨ã€‚
- è¨˜äº‹ã«å«ã¾ã‚Œãªã„æƒ…å ±ã¯å‰µä½œã—ãªã„ã“ã¨ã€‚
- ä¼šè©±ã¯ã€Œç”Ÿå¾’: ã€ã€Œå…ˆç”Ÿ: ã€ã®å½¢å¼ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚
- æœ€ä½ã§ã‚‚4å¾€å¾©ï¼ˆåˆè¨ˆ8è¡Œä»¥ä¸Šï¼‰ã®ä¼šè©±ã«ã™ã‚‹ã“ã¨ã€‚
"""
        },
        {
            "role": "user",
            "content": f"ã€è¦ç´„ã€‘\n{summary_text}\n\nã“ã®å†…å®¹ã§ä¼šè©±åŠ‡ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        }
    ]
    
    inputs2 = tokenizer.apply_chat_template(
        roleplay_prompt,
        add_generation_prompt=True,
        tokenize=True,
        return_tensors="pt"
    ).to(model.device)
    
    outputs2 = model.generate(
        inputs2,
        max_new_tokens=1024,
        temperature=0.7,
        use_cache=True
    )
    conversation_text = tokenizer.decode(
        outputs2[0][inputs2.shape[-1]:],
        skip_special_tokens=True
    )
    
    elapsed_time = time.time() - start_time
    conversation = parse_conversation(conversation_text)
    
    return {
        "summary": summary_text,
        "conversation": conversation,
        "processing_time": f"{elapsed_time:.2f} ç§’"
    }

print("âœ… ç”Ÿæˆé–¢æ•°ã®å®šç¾©å®Œäº†")

# ============================================================
# ã€ã‚»ãƒ«6ã€‘FastAPIã‚¢ãƒ—ãƒªã®å®šç¾©
# ============================================================

app = FastAPI(
    title="News to Chat API",
    description="Swallow v0.3ã‚’ä½¿ç”¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ä¼šè©±å½¢å¼ã«å¤‰æ›ã™ã‚‹API",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«
class ConvertRequest(BaseModel):
    article_id: int

class ConversationMessage(BaseModel):
    role: str
    content: str

class ConvertResponse(BaseModel):
    summary: str
    conversation: list[ConversationMessage]
    processing_time: str

class ArticleItem(BaseModel):
    id: int
    preview: str
    content: str

class ArticlesResponse(BaseModel):
    articles: list[ArticleItem]

class UploadResponse(BaseModel):
    message: str
    articles_count: int

@app.post("/upload", response_model=UploadResponse)
async def upload_csv(file: UploadFile = File(...)):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è¨˜äº‹ã‚’èª­ã¿è¾¼ã‚€"""
    global ARTICLES
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿è¾¼ã‚€
        contents = await file.read()
        
        # CSVã¨ã—ã¦è§£æ
        df = pd.read_csv(io.BytesIO(contents), encoding="utf-8")
        
        # è¨˜äº‹ãƒªã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢
        ARTICLES = []
        
        # honbunã¨midasiã‚«ãƒ©ãƒ ã®ç¢ºèª
        if "honbun" in df.columns and "midasi" in df.columns:
            for idx in range(len(df)):
                ARTICLES.append({
                    "honbun": str(df["honbun"][idx]),
                    "midasi": str(df["midasi"][idx])
                })
        elif "honbun" in df.columns:
            for idx in range(len(df)):
                content = str(df["honbun"][idx])
                ARTICLES.append({
                    "honbun": content,
                    "midasi": content[:30] + "..."
                })
        else:
            raise HTTPException(
                status_code=400,
                detail=f"'honbun'ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚«ãƒ©ãƒ : {df.columns.tolist()}"
            )
        
        return UploadResponse(
            message=f"{len(ARTICLES)}ä»¶ã®è¨˜äº‹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ",
            articles_count=len(ARTICLES)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {str(e)}")

@app.get("/articles", response_model=ArticlesResponse)
async def get_articles():
    """è¨˜äº‹ä¸€è¦§ã‚’è¿”ã™ï¼ˆpreviewã«ã¯midasiã‚’ä½¿ç”¨ï¼‰"""
    articles = []
    for i, article in enumerate(ARTICLES):
        articles.append(ArticleItem(
            id=i,
            preview=article["midasi"],
            content=article["honbun"]
        ))
    return ArticlesResponse(articles=articles)

@app.post("/convert", response_model=ConvertResponse)
async def convert_endpoint(request: ConvertRequest):
    """è¨˜äº‹ã‚’ä¼šè©±å½¢å¼ã«å¤‰æ›ã™ã‚‹"""
    try:
        result = process_article(request.article_id)
        return ConvertResponse(
            summary=result["summary"],
            conversation=[
                ConversationMessage(**msg) for msg in result["conversation"]
            ],
            processing_time=result["processing_time"]
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "articles_count": len(ARTICLES)
    }

print("âœ… FastAPIã‚¢ãƒ—ãƒªã®å®šç¾©å®Œäº†")

# ============================================================
# ã€ã‚»ãƒ«7ã€‘localtunnelã§ã‚µãƒ¼ãƒãƒ¼ã‚’å…¬é–‹ï¼ˆã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œï¼‰
# ============================================================

"""
import subprocess
import threading
import nest_asyncio
import uvicorn

# nest_asyncioã‚’é©ç”¨ï¼ˆColabç’°å¢ƒã§å¿…è¦ï¼‰
nest_asyncio.apply()

# localtunnelã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§èµ·å‹•
def start_localtunnel():
    subprocess.run(["lt", "--port", "8000"])

tunnel_thread = threading.Thread(target=start_localtunnel, daemon=True)
tunnel_thread.start()

print("ğŸŒ localtunnelã‚’èµ·å‹•ä¸­...")
print("â³ æ•°ç§’å¾Œã«URLãŒè¡¨ç¤ºã•ã‚Œã¾ã™")
print("")
print("è¡¨ç¤ºã•ã‚ŒãŸURLã‚’ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®è¨­å®šã«ä½¿ç”¨ã—ã¦ãã ã•ã„")
print("")
print("æ³¨æ„: localtunnelã¯ç™»éŒ²ä¸è¦ã§ã™ãŒã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã”ã¨ã«URLãŒå¤‰ã‚ã‚Šã¾ã™")
print("")

# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
uvicorn.run(app, host="0.0.0.0", port=8000)
"""

# ============================================================
# ä½¿ã„æ–¹ã¾ã¨ã‚
# ============================================================
print("""
============================================================
ğŸ“– ä½¿ã„æ–¹
============================================================

1. ã€ã‚»ãƒ«1ã€‘ã®pipã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼ˆåˆå›ã®ã¿ï¼‰
2. ã€ã‚»ãƒ«2ã€‘ã€œã€ã‚»ãƒ«6ã€‘ã‚’é †ç•ªã«å®Ÿè¡Œ
3. ã€ã‚»ãƒ«7ã€‘ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã—ã¦å®Ÿè¡Œ
4. è¡¨ç¤ºã•ã‚Œã‚‹localtunnel URLã‚’ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«è¨­å®š
5. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

â€» localtunnelã¯ç™»éŒ²ä¸è¦ã§ä½¿ãˆã¾ã™ï¼

============================================================
""")
