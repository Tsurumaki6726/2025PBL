# ============================================================
# News to Chat API - FastAPI Backend
# Swallow v0.3 (Llama 3.1ãƒ™ãƒ¼ã‚¹) ã‚’ä½¿ç”¨ã—ãŸä¼šè©±å¤‰æ›
# ============================================================

# --- ã‚»ãƒ«1ç›¸å½“: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
# pip install ã§ä»¥ä¸‹ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã®å‰æ:
# unsloth, xformers, trl, peft, accelerate, bitsandbytes, pandas, fastapi, uvicorn

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import time
import os

# --- ã‚»ãƒ«2ç›¸å½“: ãƒ¢ãƒ‡ãƒ«é–¢é€£ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import torch
import pandas as pd
from unsloth import FastLanguageModel

# ============================================================
# è¨­å®š
# ============================================================

# Swallow v0.3 (Llama 3.1ãƒ™ãƒ¼ã‚¹ã®æœ€æ–°æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«)
MODEL_ID = "tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3"

CSV_FILE_NAME = os.path.join(os.path.dirname(__file__), "articles", "ehime_kiji_001.csv")
COLUMN_NAME = "honbun"

# ============================================================
# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆãƒ¢ãƒ‡ãƒ«ã¨è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ï¼‰
# ============================================================

model = None
tokenizer = None
ARTICLES = []

# ============================================================
# FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
# ============================================================

app = FastAPI(
    title="News to Chat API",
    description="Swallow v0.3ã‚’ä½¿ç”¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ä¼šè©±å½¢å¼ã«å¤‰æ›ã™ã‚‹API",
    version="1.0.0"
)

# CORSè¨­å®šï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================
# ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«
# ============================================================

class ConvertRequest(BaseModel):
    article_id: int

class ConversationMessage(BaseModel):
    role: str
    content: str

class ConvertResponse(BaseModel):
    summary: str
    conversation: list[ConversationMessage]
    processing_time: str

class ArticleItem(BaseModel):
    id: int
    preview: str  # This will now hold midasi (headline)
    content: str  # honbun (article body)

class ArticlesResponse(BaseModel):
    articles: list[ArticleItem]

# ============================================================
# ã‚»ãƒ«2ç›¸å½“: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
# ============================================================

def load_articles():
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¨˜äº‹ã‚’èª­ã¿è¾¼ã‚€"""
    global ARTICLES
    
    print(f"ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªä¸­: {CSV_FILE_NAME}")
    
    if os.path.exists(CSV_FILE_NAME):
        try:
            df = pd.read_csv(CSV_FILE_NAME, encoding="utf-8")
            if "honbun" in df.columns and "midasi" in df.columns:
                ARTICLES = []
                for idx in range(len(df)):
                    ARTICLES.append({
                        "honbun": str(df["honbun"][idx]),
                        "midasi": str(df["midasi"][idx])
                    })
                print(f"âœ… CSVèª­ã¿è¾¼ã¿æˆåŠŸ: {len(ARTICLES)} ä»¶ã®è¨˜äº‹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
            elif "honbun" in df.columns:
                # Fallback: honbun only (midasi not available)
                ARTICLES = []
                for idx in range(len(df)):
                    content = str(df["honbun"][idx])
                    ARTICLES.append({
                        "honbun": content,
                        "midasi": content[:30] + "..."  # Use first 30 chars as preview
                    })
                print(f"âœ… CSVèª­ã¿è¾¼ã¿æˆåŠŸ (honbunã®ã¿): {len(ARTICLES)} ä»¶ã®è¨˜äº‹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
            else:
                print(f"âŒ åˆ—å 'honbun' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                ARTICLES = []
        except Exception as e:
            print(f"âŒ CSVã‚¨ãƒ©ãƒ¼: {e}")
            ARTICLES = []
    else:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: '{CSV_FILE_NAME}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        ARTICLES = []

# ============================================================
# ã‚»ãƒ«3ç›¸å½“: Swallowãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
# ============================================================

def load_model():
    """Swallowãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼ˆåˆå›ã®ã¿ï¼‰"""
    global model, tokenizer
    
    if model is not None:
        return True
    
    if not ARTICLES:
        print("âš ï¸ è¨˜äº‹ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return False
    
    print(f"â³ [{MODEL_ID}] ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™...")
    print("â€»Swallowã¯é«˜ç²¾åº¦ãªãŸã‚ã€åˆå›ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ãƒ­ãƒ¼ãƒ‰ã«3ã€œ5åˆ†ç¨‹åº¦ã‹ã‹ã‚Šã¾ã™ã€‚")
    
    try:
        # Unslothã‚’ä½¿ã£ã¦Swallowã‚’4bité‡å­åŒ–ã§ãƒ­ãƒ¼ãƒ‰
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=MODEL_ID,
            max_seq_length=4096,  # Swallowã¯é•·æ–‡ã«å¼·ã„ãŸã‚é•·ã‚ã«ç¢ºä¿
            dtype=None,
            load_in_4bit=True,
        )
        FastLanguageModel.for_inference(model)
        print("âœ… Swallowãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        return True
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        model = None
        tokenizer = None
        return False

# ============================================================
# ã‚»ãƒ«4ç›¸å½“: ç”Ÿæˆå‡¦ç†ï¼ˆSwallowã«æœ€é©åŒ–ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
# ============================================================

def process_article(article_id: int) -> dict:
    """
    è¨˜äº‹ã‚’è¦ç´„ã—ã€ä¼šè©±å½¢å¼ã«å¤‰æ›ã™ã‚‹
    
    Returns:
        dict: {
            "summary": str,
            "conversation": list[dict],
            "processing_time": str
        }
    """
    global model, tokenizer, ARTICLES
    
    if model is None:
        raise Exception("ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    
    if article_id < 0 or article_id >= len(ARTICLES):
        raise Exception(f"ç„¡åŠ¹ãªè¨˜äº‹ID: {article_id}")
    
    original_text = ARTICLES[article_id]["honbun"]
    start_time = time.time()
    
    # --- ã‚¹ãƒ†ãƒƒãƒ—A: è¦ç´„ ---
    summary_prompt = [
        {
            "role": "system",
            "content": "ã‚ãªãŸã¯å„ªç§€ãªç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®è¦ç‚¹ã‚’ã€äº‹å®Ÿã«åŸºã¥ã„ã¦200æ–‡å­—ç¨‹åº¦ã®æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚"
        },
        {
            "role": "user",
            "content": f"## è¨˜äº‹æœ¬æ–‡\n{original_text}"
        }
    ]
    
    inputs1 = tokenizer.apply_chat_template(
        summary_prompt,
        add_generation_prompt=True,
        tokenize=True,
        return_tensors="pt"
    ).to(model.device)
    
    outputs1 = model.generate(
        inputs1,
        max_new_tokens=300,
        temperature=0.3,  # äº‹å®Ÿé–¢ä¿‚ã‚’æ­£ç¢ºã«ã™ã‚‹ãŸã‚ä½ã‚
        use_cache=True
    )
    summary_text = tokenizer.decode(
        outputs1[0][inputs1.shape[-1]:],
        skip_special_tokens=True
    )
    
    # --- ã‚¹ãƒ†ãƒƒãƒ—B: ä¼šè©±å¤‰æ› (Swallowã®ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤èƒ½åŠ›æ´»ç”¨) ---
    roleplay_prompt = [
        {
            "role": "system",
            "content": """
ã‚ãªãŸã¯ãƒ—ãƒ­ã®è„šæœ¬å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ã€è¦ç´„ã€‘ã®å†…å®¹ã‚’ã€äºŒäººã®ç™»å ´äººç‰©ã®ä¼šè©±åŠ‡ï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼‰ã«æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚

ã€ç™»å ´äººç‰©ã€‘
1. **åšå£«**: èªå°¾ã¯ã€Œã€œã˜ã‚ƒã€ã€Œã€œã˜ã‚ƒã®ã†ã€ã‚’ä½¿ã†ã€‚çŸ¥æµè¢‹ã®ã‚ˆã†ãªè€äººã€‚
2. **ç”Ÿå¾’**: ä¸å¯§èªï¼ˆã€œã§ã™ã­ã€ã€œã§ã™ã‹ï¼Ÿï¼‰ã§è©±ã™ã€‚æ•™ãˆã‚’ä¹ã†è‹¥è€…ã€‚

ã€æ§‹æˆãƒ«ãƒ¼ãƒ«ã€‘
- æŒ¨æ‹¶ã¯çœç•¥ã—ã€ç”Ÿå¾’ãŒè¨˜äº‹ã®å†…å®¹ã«ã¤ã„ã¦è³ªå•ã™ã‚‹ã¨ã“ã‚ã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨ã€‚
- åšå£«ãŒè§£èª¬ã—ã€ç”Ÿå¾’ãŒç´å¾—ã™ã‚‹æµã‚Œã«ã™ã‚‹ã“ã¨ã€‚
- è¨˜äº‹ã«å«ã¾ã‚Œãªã„æƒ…å ±ã¯å‰µä½œã—ãªã„ã“ã¨ã€‚
- ä¼šè©±ã¯ã€Œç”Ÿå¾’: ã€ã€Œåšå£«: ã€ã®å½¢å¼ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚
- æœ€ä½ã§ã‚‚4å¾€å¾©ï¼ˆåˆè¨ˆ8è¡Œä»¥ä¸Šï¼‰ã®ä¼šè©±ã«ã™ã‚‹ã“ã¨ã€‚
"""
        },
        {
            "role": "user",
            "content": f"ã€è¦ç´„ã€‘\n{summary_text}\n\nã“ã®å†…å®¹ã§ä¼šè©±åŠ‡ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"
        }
    ]
    
    inputs2 = tokenizer.apply_chat_template(
        roleplay_prompt,
        add_generation_prompt=True,
        tokenize=True,
        return_tensors="pt"
    ).to(model.device)
    
    outputs2 = model.generate(
        inputs2,
        max_new_tokens=1024,
        temperature=0.7,  # ä¼šè©±ã®è‡ªç„¶ã•ã‚’å‡ºã™ãŸã‚å°‘ã—ä¸Šã’ã‚‹
        use_cache=True
    )
    conversation_text = tokenizer.decode(
        outputs2[0][inputs2.shape[-1]:],
        skip_special_tokens=True
    )
    
    elapsed_time = time.time() - start_time
    
    # ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒªã‚¹ãƒˆã«å¤‰æ›
    conversation = parse_conversation(conversation_text)
    
    return {
        "summary": summary_text,
        "conversation": conversation,
        "processing_time": f"{elapsed_time:.2f} ç§’"
    }


def parse_conversation(text: str) -> list[dict]:
    """
    ä¼šè©±ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
    
    ä¾‹:
    "ç”Ÿå¾’: ã“ã‚“ã«ã¡ã¯\nåšå£«: ã‚„ã‚"
    â†’ [{"role": "character_b", "content": "ã“ã‚“ã«ã¡ã¯"}, {"role": "character_a", "content": "ã‚„ã‚"}]
    """
    lines = text.strip().split("\n")
    conversation = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith("åšå£«:") or line.startswith("åšå£«ï¼š"):
            content = line.replace("åšå£«:", "").replace("åšå£«ï¼š", "").strip()
            if content:
                conversation.append({"role": "character_a", "content": content})
        elif line.startswith("ç”Ÿå¾’:") or line.startswith("ç”Ÿå¾’ï¼š"):
            content = line.replace("ç”Ÿå¾’:", "").replace("ç”Ÿå¾’ï¼š", "").strip()
            if content:
                conversation.append({"role": "character_b", "content": content})
    
    return conversation

# ============================================================
# APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ============================================================

@app.get("/articles", response_model=ArticlesResponse)
async def get_articles():
    """CSVã‹ã‚‰èª­ã¿è¾¼ã‚“ã è¨˜äº‹ä¸€è¦§ã‚’è¿”ã™"""
    articles = []
    for i, article in enumerate(ARTICLES):
        preview = article["midasi"]  # Use midasi as preview
        articles.append(ArticleItem(
            id=i,
            preview=preview,
            content=article["honbun"]
        ))
    return ArticlesResponse(articles=articles)


@app.post("/convert", response_model=ConvertResponse)
async def convert_endpoint(request: ConvertRequest):
    """è¨˜äº‹ã‚’ä¼šè©±å½¢å¼ã«å¤‰æ›ã™ã‚‹"""
    try:
        if model is None:
            raise HTTPException(
                status_code=503,
                detail="ãƒ¢ãƒ‡ãƒ«ãŒã¾ã ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„ã€‚"
            )
        
        result = process_article(request.article_id)
        
        return ConvertResponse(
            summary=result["summary"],
            conversation=[
                ConversationMessage(**msg) for msg in result["conversation"]
            ],
            processing_time=result["processing_time"]
        )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        )


@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "articles_count": len(ARTICLES)
    }

# ============================================================
# èµ·å‹•æ™‚ã®åˆæœŸåŒ–
# ============================================================

@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã¨è¨˜äº‹ã‚’ãƒ­ãƒ¼ãƒ‰"""
    print("ğŸš€ News to Chat API ã‚’èµ·å‹•ã—ã¦ã„ã¾ã™...")
    load_articles()
    load_model()
    print("âœ… èµ·å‹•å®Œäº†ï¼")

# ============================================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ============================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
